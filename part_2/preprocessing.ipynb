{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7914d547",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa76049c",
   "metadata": {},
   "source": [
    "## Section 2: File Formats & Encoding (~7 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b210b4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/data_engineer_assessment/part_2\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Show current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee139bfa",
   "metadata": {},
   "source": [
    "### Task 2.1: Reading Different File Formats\n",
    "\n",
    "The `part_2/` directory contains data in two formats:\n",
    "- `apartments_data_winterthur.csv` — apartment rental data in CSV format\n",
    "- `supermarkets.json` — supermarket locations from OpenStreetMap in JSON format\n",
    "\n",
    "**Your tasks:**\n",
    "1. Read the CSV file into a DataFrame using `pd.read_csv()` with appropriate parameters\n",
    "2. Read the JSON file into a DataFrame using `pd.read_json()`\n",
    "3. Display the first 3 rows and the shape of each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df616dbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m df_apartments = ...\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Print shape and first 3 rows\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf_apartments\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m df_apartments.head(\u001b[32m3\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ellipsis' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Task 2.1.1 — Read the CSV file\n",
    "# TODO: Read 'apartments_data_winterthur.csv' into a DataFrame\n",
    "df_apartments = ...\n",
    "\n",
    "# Print shape and first 3 rows\n",
    "print(f'Shape: {df_apartments.shape}')\n",
    "df_apartments.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1.2 — Read the JSON file\n",
    "# TODO: Read 'supermarkets.json' into a DataFrame\n",
    "df_supermarkets = ...\n",
    "\n",
    "# Print shape and first 3 rows\n",
    "print(f'Shape: {df_supermarkets.shape}')\n",
    "df_supermarkets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f06ed",
   "metadata": {},
   "source": [
    "### Task 2.2: File Format Trade-offs\n",
    "\n",
    "Answer the following questions by writing your responses as **Python comments** or in a **print()** statement.\n",
    "\n",
    "1. You need to store 50 GB of tabular sensor data that will be queried by column. Which file format would you choose: CSV, JSON, or Parquet? Why?\n",
    "\n",
    "2. You are building a REST API that returns nested, hierarchical data (e.g., a product catalog with categories and subcategories). Which format is most suitable: CSV, JSON, or XML? Why?\n",
    "\n",
    "3. Your colleague sends you a CSV file, but some columns contain nested objects (e.g., a list of tags per row). What problem does this create, and how would you handle it?\n",
    "\n",
    "4. Name one advantage of Parquet over CSV and one advantage of CSV over Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your answers as comments or print statements\n",
    "\n",
    "# Q1: 50 GB sensor data, column-based queries — which format and why?\n",
    "\n",
    "\n",
    "# Q2: REST API with nested/hierarchical data — which format and why?\n",
    "\n",
    "\n",
    "# Q3: CSV with nested objects — what's the problem and how to handle it?\n",
    "\n",
    "\n",
    "# Q4: One advantage of Parquet over CSV, and one advantage of CSV over Parquet?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1eb5e1",
   "metadata": {},
   "source": [
    "### Task 2.3: Character Encoding\n",
    "\n",
    "The apartment dataset contains Swiss German text with special characters (ä, ö, ü, etc.).\n",
    "\n",
    "**Your tasks:**\n",
    "1. Read `apartments_data_zuerich.csv` with `encoding='utf-8'` and verify that special characters display correctly\n",
    "2. Try reading the same file with `encoding='ascii'` — observe and handle the error using a `try/except` block\n",
    "3. Extract the `address_raw` column and count how many addresses contain non-ASCII characters (e.g., ü, ä, ö)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3.1 — Read with UTF-8 encoding\n",
    "# TODO: Read the Zürich apartments CSV with UTF-8 encoding\n",
    "df_zh = ...\n",
    "\n",
    "# Print the first 5 address values to verify special characters render correctly\n",
    "print(df_zh['address_raw'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b99738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3.2 — Try reading with ASCII encoding (this should fail)\n",
    "# TODO: Wrap in try/except and print the error message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875184b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3.3 — Count addresses with non-ASCII characters\n",
    "# TODO: Count how many address values contain characters like ä, ö, ü, é, etc.\n",
    "# Hint: You can use str.contains() with a regex pattern or check if a string is ASCII\n",
    "non_ascii_count = ...\n",
    "\n",
    "print(f'Addresses with non-ASCII characters: {non_ascii_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b8f73",
   "metadata": {},
   "source": [
    "### Task 2.4: File Format Conversion\n",
    "\n",
    "**Your tasks:**\n",
    "1. Take the apartments DataFrame (from Task 2.1) and write it to a **Parquet** file\n",
    "2. Read the Parquet file back and verify the data is identical\n",
    "3. Compare the file sizes of the CSV and Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb73f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.4.1 — Write to Parquet\n",
    "# TODO: Save df_apartments to 'apartments_winterthur.parquet'\n",
    "\n",
    "\n",
    "# Task 2.4.2 — Read back from Parquet and verify\n",
    "# TODO: Read the parquet file and compare shape/dtypes with the original\n",
    "df_from_parquet = ...\n",
    "\n",
    "print(f'Original shape:  {df_apartments.shape}')\n",
    "print(f'Parquet shape:   {df_from_parquet.shape}')\n",
    "print(f'DataFrames equal: {df_apartments.equals(df_from_parquet)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a60723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.4.3 — Compare file sizes\n",
    "# TODO: Use os.path.getsize() to compare the CSV and Parquet file sizes\n",
    "csv_size = ...\n",
    "parquet_size = ...\n",
    "\n",
    "print(f'CSV file size:     {csv_size:>10,} bytes')\n",
    "print(f'Parquet file size:  {parquet_size:>10,} bytes')\n",
    "print(f'Compression ratio:  {csv_size / parquet_size:.2f}x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
