{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7914d547",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa76049c",
   "metadata": {},
   "source": [
    "## Section 2: File Formats  (~7 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Show current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee139bfa",
   "metadata": {},
   "source": [
    "### Task 2.1: Reading Different File Formats\n",
    "\n",
    "The `part_2/` directory contains data in two formats:\n",
    "- `apartments_data_winterthur.csv` — apartment rental data in CSV format\n",
    "- `supermarkets.json` — supermarket locations from OpenStreetMap in JSON format\n",
    "\n",
    "**Your tasks:**\n",
    "1. Read the CSV file into a DataFrame using `pd.read_csv()` with appropriate parameters\n",
    "2. Read the JSON file into a DataFrame using `pd.read_json()`\n",
    "3. Display the first 3 rows and the shape of each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df616dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1.1 — Read the CSV file\n",
    "# TODO: Read 'apartments_data_winterthur.csv' into a DataFrame\n",
    "df_apartments = pd.read_csv('apartments_data_winterthur.csv')\n",
    "\n",
    "# Print info and first 3 rows\n",
    "print(f'Info: {df_apartments.info()}')\n",
    "\n",
    "df_apartments.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1.2 — Read the JSON file\n",
    "# TODO: Read 'supermarkets.json' into a DataFrame\n",
    "df_supermarkets = pd.read_json('supermarkets.json')\n",
    "\n",
    "# Print last 3 rows\n",
    "df_supermarkets.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffb8a6",
   "metadata": {},
   "source": [
    "### Task 2.2: Nested Objects\n",
    "\n",
    "The `df_supermarkets` DataFrame has a nested object in the `tags` column. Each row contains a dictionary with multiple OSM (OpenStreetMap) attributes like brand, opening hours, address details, etc.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Inspect the `tags` column to understand its structure (display one example)\n",
    "2. Flatten the nested `tags` dictionary into separate columns using `pd.json_normalize()`\n",
    "3. Combine the flattened columns with the original location columns (`type`, `id`, `lat`, `lon`)\n",
    "4. Drop the original `tags` column and display the resulting DataFrame\n",
    "5. Compare the shape before and after flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.2 Solution — Flattening Nested Objects\n",
    "\n",
    "#TODO Step 1: Inspect the tags column structure\n",
    "print(\"Step 1: Inspect one example of the nested 'tags' column\")\n",
    "print(f\"Type of tags[0]: {type(df_supermarkets['tags'].iloc[0])}\")\n",
    "print(f\"\\nExample tags dictionary (first row):\")\n",
    "print(df_supermarkets['tags'].iloc[0])\n",
    "print(f\"\\nNumber of keys in first row: {len(df_supermarkets['tags'].iloc[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 & 3: Flatten the tags columns and combine with original columns\n",
    "\n",
    "#TODO Flatten the tags dictionary into separate columns\n",
    "tags_normalized = pd.json_normalize(df_supermarkets['tags'])\n",
    "\n",
    "#TODO Combine with the original location columns\n",
    "df_supermarkets_flattened = pd.concat(\n",
    "    [df_supermarkets[['type', 'id', 'lat', 'lon']], tags_normalized],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Original shape: {df_supermarkets.shape}\")\n",
    "print(f\"Flattened shape: {df_supermarkets_flattened.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13948a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Step 4: Display information about the flattened DataFrame\n",
    "print(\"Column names:\")\n",
    "print(df_supermarkets_flattened.columns.tolist())\n",
    "print(f\"\\nFirst 3 rows of flattened data:\")\n",
    "print(df_supermarkets_flattened.head(3))\n",
    "print(f\"\\nData types:\")\n",
    "print(df_supermarkets_flattened.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b8f73",
   "metadata": {},
   "source": [
    "### Task 2.3: File Format Conversion\n",
    "\n",
    "**Your tasks:**\n",
    "1. Take the apartments DataFrame (from Task 2.1) and write it to a **Parquet** file\n",
    "2. Read the Parquet file back and verify the data is identical\n",
    "3. Compare the file sizes of the CSV and Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb73f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3.1 — Write to Parquet\n",
    "# TODO: Save df_apartments to 'apartments_winterthur.parquet'\n",
    "df_apartments.to_parquet('apartments_winterthur.parquet', index=False)\n",
    "\n",
    "# Task 2.3.2 — Read back from Parquet and verify\n",
    "# TODO: Read the parquet file and compare shape/dtypes with the original\n",
    "df_from_parquet = pd.read_parquet('apartments_winterthur.parquet')\n",
    "\n",
    "print(f'Original shape:  {df_apartments.shape}')\n",
    "print(f'Parquet shape:   {df_from_parquet.shape}')\n",
    "print(f'DataFrames equal: {df_apartments.equals(df_from_parquet)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a60723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3.3 — Compare file sizes\n",
    "# TODO: Use os.path.getsize() to compare the CSV and Parquet file sizes\n",
    "csv_size = os.path.getsize('apartments_data_winterthur.csv')\n",
    "parquet_size = os.path.getsize('apartments_winterthur.parquet')\n",
    "\n",
    "print(f'CSV file size:     {csv_size:>10,} bytes')\n",
    "print(f'Parquet file size:  {parquet_size:>10,} bytes')\n",
    "print(f'Compression ratio:  {csv_size / parquet_size:.2f}x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
